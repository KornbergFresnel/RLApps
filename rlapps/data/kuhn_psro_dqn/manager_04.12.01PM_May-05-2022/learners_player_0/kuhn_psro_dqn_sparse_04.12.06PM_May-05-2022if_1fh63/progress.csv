episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,num_healthy_workers,timesteps_total,timesteps_this_iter,agent_timesteps_total,done,episodes_total,training_iteration,trial_id,experiment_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,timesteps_since_restore,iterations_since_restore,warmup_time,scenario_name,policy_reward_min/metanash,policy_reward_min/best_response,policy_reward_max/metanash,policy_reward_max/best_response,policy_reward_mean/metanash,policy_reward_mean/best_response,sampler_perf/mean_raw_obs_processing_ms,sampler_perf/mean_inference_ms,sampler_perf/mean_action_processing_ms,sampler_perf/mean_env_wait_ms,sampler_perf/mean_env_render_ms,timers/load_time_ms,timers/load_throughput,timers/learn_time_ms,timers/learn_throughput,timers/update_time_ms,info/num_steps_sampled,info/num_agent_steps_sampled,info/num_steps_trained,info/num_steps_trained_this_iter,info/num_agent_steps_trained,info/last_target_update_ts,info/num_target_updates,info/learner/best_response/loss,info/learner/best_response/num_agent_steps_trained,info/learner/best_response/learner_stats/allreduce_latency
0.0,0.0,0.0,2.048,498,4,25600,4096,25598,False,12412,10,default,00b4850d875c4155b46540135895496a,2022-05-05_16-12-16,1651738336,0.4059028625488281,8.989879608154297,2849116,sjtu-marl1,192.168.2.51,8.989879608154297,40960,10,1.4438800811767578,kuhn_psro_dqn,-2.0,-2.0,2.0,2.0,-1.244,1.244,0.2357308836922045,0.4284453924095178,0.026832100493594843,0.08135729766297743,0.0,0.133,30910164.059,6.247,655725.203,1.169,25600,25598,40960,4096,81920,16384,1,0.4833259880542755,4096.0,0.0
