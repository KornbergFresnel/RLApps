episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,num_healthy_workers,timesteps_total,timesteps_this_iter,agent_timesteps_total,done,episodes_total,training_iteration,trial_id,experiment_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,timesteps_since_restore,iterations_since_restore,warmup_time,scenario_name,policy_reward_min/best_response,policy_reward_min/metanash,policy_reward_max/best_response,policy_reward_max/metanash,policy_reward_mean/best_response,policy_reward_mean/metanash,sampler_perf/mean_raw_obs_processing_ms,sampler_perf/mean_inference_ms,sampler_perf/mean_action_processing_ms,sampler_perf/mean_env_wait_ms,sampler_perf/mean_env_render_ms,timers/load_time_ms,timers/load_throughput,timers/learn_time_ms,timers/learn_throughput,timers/update_time_ms,info/num_steps_sampled,info/num_agent_steps_sampled,info/num_steps_trained,info/num_steps_trained_this_iter,info/num_agent_steps_trained,info/last_target_update_ts,info/num_target_updates,perf/cpu_util_percent,perf/ram_util_percent,info/learner/best_response/loss,info/learner/best_response/num_agent_steps_trained,info/learner/best_response/learner_stats/allreduce_latency
0.0,0.0,0.0,2.0,512,4,25600,4096,25600,False,12800,10,default,9956fe5526b847dca53011a2a3a9bd09,2022-05-05_16-13-32,1651738412,0.41535019874572754,9.134471654891968,2850730,sjtu-marl1,192.168.2.51,9.134471654891968,40960,10,1.4367287158966064,kuhn_psro_dqn,-2.0,-2.0,2.0,2.0,-0.079,0.079,0.24366615126890653,0.43395455258717824,0.027026700988136816,0.08320557792475254,0.0,0.133,30782779.402,5.96,687222.256,1.244,25600,25600,40960,4096,81920,16384,1,4.3,4.2,1.4994786977767944,4096.0,0.0
