episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,num_healthy_workers,timesteps_total,timesteps_this_iter,agent_timesteps_total,done,episodes_total,training_iteration,trial_id,experiment_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,timesteps_since_restore,iterations_since_restore,warmup_time,scenario_name,policy_reward_min/metanash,policy_reward_min/best_response,policy_reward_max/metanash,policy_reward_max/best_response,policy_reward_mean/metanash,policy_reward_mean/best_response,sampler_perf/mean_raw_obs_processing_ms,sampler_perf/mean_inference_ms,sampler_perf/mean_action_processing_ms,sampler_perf/mean_env_wait_ms,sampler_perf/mean_env_render_ms,timers/load_time_ms,timers/load_throughput,timers/learn_time_ms,timers/learn_throughput,timers/update_time_ms,info/num_steps_sampled,info/num_agent_steps_sampled,info/num_steps_trained,info/num_steps_trained_this_iter,info/num_agent_steps_trained,info/last_target_update_ts,info/num_target_updates,perf/cpu_util_percent,perf/ram_util_percent,info/learner/best_response/loss,info/learner/best_response/num_agent_steps_trained,info/learner/best_response/learner_stats/allreduce_latency
0.0,0.0,0.0,2.0,512,4,25600,4096,25600,False,12800,10,default,1faf6e23d3604a928b65056b6ad4cbe9,2022-05-05_16-13-29,1651738409,0.4289560317993164,9.283754825592041,2850530,sjtu-marl1,192.168.2.51,9.283754825592041,40960,10,1.4784553050994873,kuhn_psro_dqn,-1.0,-1.0,1.0,1.0,-0.932,0.932,0.24772722305068115,0.4365818487066631,0.02751966454656614,0.08315785927990875,0.0,0.139,29447838.848,6.834,599370.244,1.317,25600,25600,40960,4096,81920,16384,1,7.4,4.6,0.23689435422420502,4096.0,0.0
